{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "from torch.optim.adam import Adam\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cfgs = {\n",
    "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "pretrained_vgg = utils.VGG(cfgs['VGG16'])\n",
    "pretrained_vgg.load_state_dict(torch.load('vgg16-89.33%.pth')) # type: ignore\n",
    "\n",
    "\n",
    "bayesian_vgg = utils.BayesianVGG(cfgs['VGG16'])\n",
    "bayesian_vgg.from_vgg(pretrained_vgg)\n",
    "\n",
    "\n",
    "optimizer = Adam(bayesian_vgg.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=25, gamma=0.6)\n",
    "\n",
    "\n",
    "def reg_coef_lambda(epoch: int) -> float:\n",
    "    return min(1., 0.5 ** (8 - epoch // 50))\n",
    "\n",
    "\n",
    "train_loader, test_loader = utils.get_cifar_dataloaders()\n",
    "\n",
    "\n",
    "trainer = utils.BayesianModelTrainer(bayesian_vgg, train_loader, test_loader, optimizer, scheduler, epochs=500, reg_coef_lambda=reg_coef_lambda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mantonii-belyshev\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/coder/project/wandb/run-20240828_053139-1exhidcg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/antonii-belyshev/neuron%20sparsity/runs/1exhidcg' target=\"_blank\">default experiment</a></strong> to <a href='https://wandb.ai/antonii-belyshev/neuron%20sparsity' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/antonii-belyshev/neuron%20sparsity' target=\"_blank\">https://wandb.ai/antonii-belyshev/neuron%20sparsity</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/antonii-belyshev/neuron%20sparsity/runs/1exhidcg' target=\"_blank\">https://wandb.ai/antonii-belyshev/neuron%20sparsity/runs/1exhidcg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▃▆▆▇▇███▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>eval_loss</td><td>▁▄▆▆▇▇▆▇██▇███▇▇▆▇▆▆▇▆▅▅▆▆▆▆▆▆▆▆▇▆▇▆▆▆▇▆</td></tr><tr><td>loss</td><td>▄▁▁▁▂▃▁▁▁▂▂▁▂▂▁▁▁▂▂▁▅▃▄▂▄▂▂▅▅▄▅▅▅█▄▅▆▄▅▆</td></tr><tr><td>neuron_sparsity_0</td><td>▁▁▁▁▁▁▁▁▁▁▁▂▄▄▄▇▇▇▇▇████████████████████</td></tr><tr><td>neuron_sparsity_1</td><td>▅▂▂▂▁▃▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇████████████████</td></tr><tr><td>neuron_sparsity_10</td><td>▁▃▆▃▅▃▄▅▆▇██████████████████████████████</td></tr><tr><td>neuron_sparsity_11</td><td>▁▄▇▄▇▇▇▇████████████████████████████████</td></tr><tr><td>neuron_sparsity_12</td><td>▁▁▂▁▃▄▄▄▄▅▅▅▆▆▇▇▇▇██████████████████████</td></tr><tr><td>neuron_sparsity_2</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁████████████████████</td></tr><tr><td>neuron_sparsity_3</td><td>▁▂▂▁▅▁▇█████████████████████████████████</td></tr><tr><td>neuron_sparsity_4</td><td>▁▁▁▁▁▁▁▁▁▁▃▃▅▅▅▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇███████</td></tr><tr><td>neuron_sparsity_5</td><td>▁▁▁▁▁▁▂▂▃▄▄▅▅▆▇▇▇███████████████████████</td></tr><tr><td>neuron_sparsity_6</td><td>▂▃▃▁▅▂▅▅▆▆▇▇▇▇▇▇▇▇██████████████████████</td></tr><tr><td>neuron_sparsity_7</td><td>▁▁▂▂▃▂▄▄▅▆▆▇▇▇▇█████████████████████████</td></tr><tr><td>neuron_sparsity_8</td><td>▁▂▃▂▃▄▆▇████████████████████████████████</td></tr><tr><td>neuron_sparsity_9</td><td>▁▅▇▄█▆▇▇████████████████████████████████</td></tr><tr><td>reg</td><td>█▅▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sparsity_0</td><td>▁▁▂▂▂▂▂▂▂▄▄▅▅▆▇▇▇▇▇▇▇▇▇▇████████████████</td></tr><tr><td>sparsity_1</td><td>▁▁▂▂▂▄▄▄▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇█████████████████</td></tr><tr><td>sparsity_10</td><td>▁▅▇▆▇▇▇█████████████████████████████████</td></tr><tr><td>sparsity_11</td><td>▁▆█▆█▇██████████████████████████████████</td></tr><tr><td>sparsity_12</td><td>▁▆▇▄▇▇██████████████████████████████████</td></tr><tr><td>sparsity_2</td><td>▁▁▂▁▂▄▄▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇███████████████</td></tr><tr><td>sparsity_3</td><td>▁▂▃▃▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇██████████████</td></tr><tr><td>sparsity_4</td><td>▁▂▂▂▃▃▄▄▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇███████████████</td></tr><tr><td>sparsity_5</td><td>▁▁▂▂▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇███████████████</td></tr><tr><td>sparsity_6</td><td>▁▂▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇█████████████████</td></tr><tr><td>sparsity_7</td><td>▁▂▃▃▄▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇████████████████████</td></tr><tr><td>sparsity_8</td><td>▁▄▆▆▇▇▇▇████████████████████████████████</td></tr><tr><td>sparsity_9</td><td>▁▆▇▆▇▇██████████████████████████████████</td></tr><tr><td>total_neuron_sparsity</td><td>▁▃▅▃▅▄▅▆▇▇▇▇████████████████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>88.57</td></tr><tr><td>epoch</td><td>499</td></tr><tr><td>eval_loss</td><td>0.65286</td></tr><tr><td>loss</td><td>0.14185</td></tr><tr><td>neuron_sparsity_0</td><td>0.07812</td></tr><tr><td>neuron_sparsity_1</td><td>0.17188</td></tr><tr><td>neuron_sparsity_10</td><td>0.96875</td></tr><tr><td>neuron_sparsity_11</td><td>0.9707</td></tr><tr><td>neuron_sparsity_12</td><td>0.78516</td></tr><tr><td>neuron_sparsity_2</td><td>0.00781</td></tr><tr><td>neuron_sparsity_3</td><td>0.08594</td></tr><tr><td>neuron_sparsity_4</td><td>0.05469</td></tr><tr><td>neuron_sparsity_5</td><td>0.07422</td></tr><tr><td>neuron_sparsity_6</td><td>0.16016</td></tr><tr><td>neuron_sparsity_7</td><td>0.26172</td></tr><tr><td>neuron_sparsity_8</td><td>0.72852</td></tr><tr><td>neuron_sparsity_9</td><td>0.85156</td></tr><tr><td>reg</td><td>5.63026</td></tr><tr><td>sparsity_0</td><td>0.1169</td></tr><tr><td>sparsity_1</td><td>0.34825</td></tr><tr><td>sparsity_10</td><td>0.99613</td></tr><tr><td>sparsity_11</td><td>0.99918</td></tr><tr><td>sparsity_12</td><td>0.99828</td></tr><tr><td>sparsity_2</td><td>0.27992</td></tr><tr><td>sparsity_3</td><td>0.22127</td></tr><tr><td>sparsity_4</td><td>0.3357</td></tr><tr><td>sparsity_5</td><td>0.4207</td></tr><tr><td>sparsity_6</td><td>0.39773</td></tr><tr><td>sparsity_7</td><td>0.56074</td></tr><tr><td>sparsity_8</td><td>0.87693</td></tr><tr><td>sparsity_9</td><td>0.97009</td></tr><tr><td>total_neuron_sparsity</td><td>57.76515</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">default experiment</strong> at: <a href='https://wandb.ai/antonii-belyshev/neuron%20sparsity/runs/1exhidcg' target=\"_blank\">https://wandb.ai/antonii-belyshev/neuron%20sparsity/runs/1exhidcg</a><br/> View project at: <a href='https://wandb.ai/antonii-belyshev/neuron%20sparsity' target=\"_blank\">https://wandb.ai/antonii-belyshev/neuron%20sparsity</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240828_053139-1exhidcg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(bayesian_vgg.state_dict(), 'bayesian-vgg16.pth') # type: ignore"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
