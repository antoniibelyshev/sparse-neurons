{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torch.optim.adam import Adam\n",
    "\n",
    "from torchvision.datasets import MNIST # type: ignore\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize # type: ignore\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FlattenTransform:\n",
    "#     def __call__(self, x: Tensor) -> Tensor:\n",
    "#         return x.view(-1)\n",
    "\n",
    "\n",
    "# transform = Compose([ # type: ignore\n",
    "#     ToTensor(),\n",
    "#     Normalize((0.1307,), (0.3081,)),\n",
    "#     FlattenTransform(),\n",
    "# ])\n",
    "\n",
    "# train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "# test_dataset = MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "# train_loader: DataLoader[tuple[Tensor, Tensor]] = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "# test_loader: DataLoader[tuple[Tensor, Tensor]] = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# lenet = utils.LeNet()\n",
    "\n",
    "# optimizer = Adam(lenet.parameters(), lr=1e-3)\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = utils.Trainer(lenet, train_loader, test_loader, optimizer, scheduler, epochs=100)\n",
    "# trainer.train(name=\"lenet mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mantonii-belyshev\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/coder/project/wandb/run-20240823_174111-zm8owfm4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/antonii-belyshev/neuron%20sparsity/runs/zm8owfm4' target=\"_blank\">bayesian lenet mnist</a></strong> to <a href='https://wandb.ai/antonii-belyshev/neuron%20sparsity' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/antonii-belyshev/neuron%20sparsity' target=\"_blank\">https://wandb.ai/antonii-belyshev/neuron%20sparsity</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/antonii-belyshev/neuron%20sparsity/runs/zm8owfm4' target=\"_blank\">https://wandb.ai/antonii-belyshev/neuron%20sparsity/runs/zm8owfm4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▇▇▇▇▇███▇█▇▇▆▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>eval_loss</td><td>▇▄█▅▂▂▁▁▁▂▂▂▂▃▂▂▂▃▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>loss</td><td>▃▁▂▁▁▁▂▄▃▁▅▂▂▃▂▃▃█▂▃▄▄▃▃▄▃▃▃▃▄▁▃▇▃▄▄▃▂▃▂</td></tr><tr><td>neuron_sparsity_0</td><td>▁▁▁▁▂▃▃▄▅▅▆▆▇▇▇█████████████████████████</td></tr><tr><td>neuron_sparsity_1</td><td>▁▁▁▁▁▁▁▁▂▂▃▄▅▆▆▇▇▇▇▇▇▇██████████████████</td></tr><tr><td>neuron_sparsity_2</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>reg</td><td>▇▇█▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sparsity_0</td><td>▁▁▁▄▆▆▇▇████████████████████████████████</td></tr><tr><td>sparsity_1</td><td>▁▁▂▃▅▆▇▇▇▇██████████████████████████████</td></tr><tr><td>sparsity_2</td><td>▁▁▁▁▂▂▃▃▄▄▄▅▅▆▆▆▇▇▇▇▇▇██████████████████</td></tr><tr><td>total_neuron_sparsity</td><td>▁▁▁▁▂▂▃▃▄▄▅▆▆▇▇▇▇▇██████████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>98.08</td></tr><tr><td>epoch</td><td>199</td></tr><tr><td>eval_loss</td><td>0.06475</td></tr><tr><td>loss</td><td>0.0165</td></tr><tr><td>neuron_sparsity_0</td><td>0.904</td></tr><tr><td>neuron_sparsity_1</td><td>0.8</td></tr><tr><td>neuron_sparsity_2</td><td>0.0</td></tr><tr><td>reg</td><td>0.13598</td></tr><tr><td>sparsity_0</td><td>0.96276</td></tr><tr><td>sparsity_1</td><td>0.98596</td></tr><tr><td>sparsity_2</td><td>0.825</td></tr><tr><td>total_neuron_sparsity</td><td>85.4321</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">bayesian lenet mnist</strong> at: <a href='https://wandb.ai/antonii-belyshev/neuron%20sparsity/runs/zm8owfm4' target=\"_blank\">https://wandb.ai/antonii-belyshev/neuron%20sparsity/runs/zm8owfm4</a><br/> View project at: <a href='https://wandb.ai/antonii-belyshev/neuron%20sparsity' target=\"_blank\">https://wandb.ai/antonii-belyshev/neuron%20sparsity</a><br/>Synced 5 W&B file(s), 60 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240823_174111-zm8owfm4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "bayesian_lenet = utils.BayesianLeNet()\n",
    "\n",
    "optimizer = Adam(bayesian_lenet.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.98)\n",
    "\n",
    "def reg_coef_lambda(epoch: int) -> float:\n",
    "    if epoch < 10:\n",
    "        return 0\n",
    "    elif epoch < 60:\n",
    "        return 2 ** ((epoch - 10) / 50) - 1\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "train_loader, test_loader = utils.get_mnist_dataloaders()\n",
    "\n",
    "trainer = utils.BayesianModelTrainer(bayesian_lenet, train_loader, test_loader, optimizer, scheduler, epochs=200, reg_coef_lambda=reg_coef_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(name=\"bayesian lenet mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
